# Server Configuration
PORT=5050

# LLM Provider: "groq" (cloud) or "ollama" (local)
# Defaults to "ollama" in dev, "groq" when NODE_ENV=production
LLM_PROVIDER=ollama

# Groq Configuration (when LLM_PROVIDER=groq)
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.3-70b-versatile

# Available Groq Models:
# - deepseek-r1-distill-llama-70b (powerful reasoning)
# - llama-3.3-70b-versatile (versatile, high quality)
# - llama-3.1-8b-instant (fast, lightweight)
# - mixtral-8x7b-32768 (good balance of speed and quality)
# - gemma2-9b-it (Google's compact model)
# See https://console.groq.com/docs/models for all available models

# Ollama Configuration (when LLM_PROVIDER=ollama)
# Requires Ollama installed and running: https://ollama.ai
# OLLAMA_API_URL=http://localhost:11434/v1  (default)
OLLAMA_MODEL=qwen3-coder:30b

# GitHub Configuration (optional, for higher rate limits)
GITHUB_TOKEN=your_github_token_here
