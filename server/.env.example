# Server Configuration
PORT=5001

# Ollama Configuration
OLLAMA_API_URL=http://localhost:11434/api
OLLAMA_MODEL=mistral

# Available Ollama Models:
# - mistral (default, fast and good quality)
# - llama2 (good quality, larger)
# - neural-chat (optimized for conversation)
# - dolphin-mixtral (powerful, larger)
# See https://ollama.ai/library for more models

# GitHub Configuration (optional, for higher rate limits)
GITHUB_TOKEN=your_github_token_here
